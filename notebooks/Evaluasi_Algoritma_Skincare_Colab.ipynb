{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üß¥ Evaluasi Algoritma Rekomendasi Skincare\n",
    "\n",
    "## üìã Tujuan Evaluasi\n",
    "Notebook ini akan mengevaluasi performa algoritma rekomendasi skincare menggunakan:\n",
    "- **Content-Based Filtering (CBF)** dengan TF-IDF\n",
    "- **K-Nearest Neighbors (KNN)** enhancement\n",
    "- **Hybrid System** CBF + Aggressive KNN\n",
    "\n",
    "## üìä Metrik Evaluasi\n",
    "- **Precision@K**: Akurasi rekomendasi\n",
    "- **Recall@K**: Kelengkapan rekomendasi\n",
    "- **F1-Score@K**: Keseimbangan precision dan recall\n",
    "- **NDCG@K**: Kualitas ranking\n",
    "- **MAP**: Mean Average Precision\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1_header"
   },
   "source": [
    "## üöÄ Langkah 1: Setup Environment\n",
    "\n",
    "Pertama, kita akan menginstall dan mengimport semua library yang diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_libraries"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install numpy pandas scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üì¶ NumPy version: {np.__version__}\")\n",
    "print(f\"üì¶ Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1_checkpoint"
   },
   "source": [
    "### ‚úÖ **Checkpoint 1**: Environment Ready!\n",
    "Jika tidak ada error di atas, environment sudah siap untuk digunakan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2_header"
   },
   "source": [
    "## üìä Langkah 2: Persiapan Data\n",
    "\n",
    "Kita akan membuat dataset sintetis untuk simulasi evaluasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_products"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic product data\n",
    "def generate_product_data(n_products=50):\n",
    "    \"\"\"\n",
    "    Generate synthetic skincare product data\n",
    "    \"\"\"\n",
    "    # Skincare categories and ingredients\n",
    "    categories = ['cleanser', 'moisturizer', 'serum', 'sunscreen', 'toner', 'mask']\n",
    "    skin_types = ['oily', 'dry', 'combination', 'sensitive', 'normal']\n",
    "    concerns = ['acne', 'aging', 'brightening', 'hydration', 'pores', 'dark_spots']\n",
    "    ingredients = ['niacinamide', 'hyaluronic_acid', 'vitamin_c', 'retinol', 'salicylic_acid', \n",
    "                  'ceramide', 'peptides', 'glycolic_acid', 'zinc', 'tea_tree']\n",
    "    \n",
    "    products = []\n",
    "    for i in range(n_products):\n",
    "        # Random product attributes\n",
    "        category = np.random.choice(categories)\n",
    "        suitable_skin = np.random.choice(skin_types, size=np.random.randint(1, 3), replace=False)\n",
    "        target_concerns = np.random.choice(concerns, size=np.random.randint(1, 3), replace=False)\n",
    "        product_ingredients = np.random.choice(ingredients, size=np.random.randint(2, 5), replace=False)\n",
    "        \n",
    "        # Create description\n",
    "        description = f\"{category} for {' '.join(suitable_skin)} skin targeting {' '.join(target_concerns)} with {' '.join(product_ingredients)}\"\n",
    "        \n",
    "        products.append({\n",
    "            'product_id': f'P{i+1:03d}',\n",
    "            'name': f'{category.title()} {i+1}',\n",
    "            'category': category,\n",
    "            'skin_type': ' '.join(suitable_skin),\n",
    "            'concerns': ' '.join(target_concerns),\n",
    "            'ingredients': ' '.join(product_ingredients),\n",
    "            'description': description,\n",
    "            'price': np.random.randint(50, 500) * 1000,  # Price in IDR\n",
    "            'rating': round(np.random.uniform(3.5, 5.0), 1)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(products)\n",
    "\n",
    "# Generate products\n",
    "products_df = generate_product_data(50)\n",
    "print(f\"‚úÖ Generated {len(products_df)} products\")\n",
    "print(\"\\nüìã Sample products:\")\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_users"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic user data\n",
    "def generate_user_data(n_users=20):\n",
    "    \"\"\"\n",
    "    Generate synthetic user preference data\n",
    "    \"\"\"\n",
    "    skin_types = ['oily', 'dry', 'combination', 'sensitive', 'normal']\n",
    "    concerns = ['acne', 'aging', 'brightening', 'hydration', 'pores', 'dark_spots']\n",
    "    \n",
    "    users = []\n",
    "    for i in range(n_users):\n",
    "        # Random user preferences\n",
    "        user_skin_type = np.random.choice(skin_types)\n",
    "        user_concerns = np.random.choice(concerns, size=np.random.randint(1, 4), replace=False)\n",
    "        \n",
    "        # Create user profile description\n",
    "        profile = f\"{user_skin_type} skin with concerns about {' '.join(user_concerns)}\"\n",
    "        \n",
    "        users.append({\n",
    "            'user_id': f'U{i+1:03d}',\n",
    "            'skin_type': user_skin_type,\n",
    "            'concerns': ' '.join(user_concerns),\n",
    "            'profile': profile,\n",
    "            'age': np.random.randint(18, 50),\n",
    "            'budget': np.random.choice(['low', 'medium', 'high'])\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "# Generate users\n",
    "users_df = generate_user_data(20)\n",
    "print(f\"‚úÖ Generated {len(users_df)} users\")\n",
    "print(\"\\nüë• Sample users:\")\n",
    "users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2_checkpoint"
   },
   "source": [
    "### ‚úÖ **Checkpoint 2**: Data Ready!\n",
    "Dataset sintetis telah dibuat:\n",
    "- **50 produk skincare** dengan berbagai kategori dan ingredients\n",
    "- **20 pengguna** dengan preferensi yang beragam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3_header"
   },
   "source": [
    "## ü§ñ Langkah 3: Implementasi Algoritma Rekomendasi\n",
    "\n",
    "Sekarang kita akan mengimplementasikan sistem rekomendasi hybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "recommendation_system"
   },
   "outputs": [],
   "source": [
    "class SkincareRecommendationSystem:\n",
    "    def __init__(self, products_df, users_df):\n",
    "        self.products_df = products_df.copy()\n",
    "        self.users_df = users_df.copy()\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.product_vectors = None\n",
    "        self.user_vectors = None\n",
    "        \n",
    "    def prepare_features(self):\n",
    "        \"\"\"\n",
    "        Prepare TF-IDF features for products and users\n",
    "        \"\"\"\n",
    "        # Combine all text features for products\n",
    "        self.products_df['combined_features'] = (\n",
    "            self.products_df['category'] + ' ' +\n",
    "            self.products_df['skin_type'] + ' ' +\n",
    "            self.products_df['concerns'] + ' ' +\n",
    "            self.products_df['ingredients']\n",
    "        )\n",
    "        \n",
    "        # Combine all text features for users\n",
    "        self.users_df['combined_features'] = (\n",
    "            self.users_df['skin_type'] + ' ' +\n",
    "            self.users_df['concerns']\n",
    "        )\n",
    "        \n",
    "        # Create TF-IDF vectors\n",
    "        all_features = list(self.products_df['combined_features']) + list(self.users_df['combined_features'])\n",
    "        \n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        \n",
    "        all_vectors = self.tfidf_vectorizer.fit_transform(all_features)\n",
    "        \n",
    "        # Split vectors back to products and users\n",
    "        n_products = len(self.products_df)\n",
    "        self.product_vectors = all_vectors[:n_products]\n",
    "        self.user_vectors = all_vectors[n_products:]\n",
    "        \n",
    "        print(f\"‚úÖ TF-IDF features prepared: {self.product_vectors.shape[1]} features\")\n",
    "    \n",
    "    def content_based_filtering(self, user_idx, top_k=10):\n",
    "        \"\"\"\n",
    "        Content-Based Filtering using TF-IDF and cosine similarity\n",
    "        \"\"\"\n",
    "        if self.product_vectors is None:\n",
    "            self.prepare_features()\n",
    "        \n",
    "        # Calculate similarity between user and all products\n",
    "        user_vector = self.user_vectors[user_idx]\n",
    "        similarities = cosine_similarity(user_vector, self.product_vectors).flatten()\n",
    "        \n",
    "        # Get top-k products\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'product_id': self.products_df.iloc[idx]['product_id'],\n",
    "                'name': self.products_df.iloc[idx]['name'],\n",
    "                'similarity_score': similarities[idx],\n",
    "                'product_idx': idx\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def find_similar_users(self, user_idx, k=5):\n",
    "        \"\"\"\n",
    "        Find K most similar users using cosine similarity\n",
    "        \"\"\"\n",
    "        if self.user_vectors is None:\n",
    "            self.prepare_features()\n",
    "        \n",
    "        # Calculate similarity between users\n",
    "        user_vector = self.user_vectors[user_idx]\n",
    "        similarities = cosine_similarity(user_vector, self.user_vectors).flatten()\n",
    "        \n",
    "        # Exclude self and get top-k\n",
    "        similarities[user_idx] = -1  # Exclude self\n",
    "        top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "        \n",
    "        return top_k_indices, similarities[top_k_indices]\n",
    "    \n",
    "    def aggressive_knn_scoring(self, cbf_scores, similar_users, k):\n",
    "        \"\"\"\n",
    "        Apply aggressive KNN scoring with K-dependent parameters\n",
    "        \"\"\"\n",
    "        # K-dependent parameters\n",
    "        if k <= 3:\n",
    "            position_multiplier = 0.3\n",
    "            exclusivity_bonus = 0.4\n",
    "            exclusion_penalty = 0.6\n",
    "        elif k <= 5:\n",
    "            position_multiplier = 0.25\n",
    "            exclusivity_bonus = 0.3\n",
    "            exclusion_penalty = 0.4\n",
    "        else:\n",
    "            position_multiplier = 0.2\n",
    "            exclusivity_bonus = 0.2\n",
    "            exclusion_penalty = 0.2\n",
    "        \n",
    "        enhanced_scores = cbf_scores.copy()\n",
    "        \n",
    "        # Simulate KNN recommendations for similar users\n",
    "        for i, similar_user_idx in enumerate(similar_users):\n",
    "            # Get CBF recommendations for similar user\n",
    "            similar_user_recs = self.content_based_filtering(similar_user_idx, top_k=len(cbf_scores))\n",
    "            \n",
    "            for j, rec in enumerate(similar_user_recs):\n",
    "                product_idx = rec['product_idx']\n",
    "                \n",
    "                if j < k:  # Product is in top-K for similar user\n",
    "                    # Position-based boost\n",
    "                    position_boost = position_multiplier * (k - j) / k\n",
    "                    \n",
    "                    # Exclusivity bonus (if product appears in multiple similar users' top-K)\n",
    "                    exclusivity = exclusivity_bonus * (i + 1) / len(similar_users)\n",
    "                    \n",
    "                    # Apply boost\n",
    "                    boost = position_boost + exclusivity\n",
    "                    enhanced_scores[product_idx] += boost\n",
    "                else:\n",
    "                    # Exclusion penalty for products not in top-K\n",
    "                    penalty = exclusion_penalty * 0.1\n",
    "                    enhanced_scores[product_idx] = max(0, enhanced_scores[product_idx] - penalty)\n",
    "        \n",
    "        return enhanced_scores\n",
    "    \n",
    "    def hybrid_recommend(self, user_idx, k=5, top_n=10):\n",
    "        \"\"\"\n",
    "        Hybrid recommendation combining CBF and aggressive KNN\n",
    "        \"\"\"\n",
    "        # Get CBF recommendations\n",
    "        cbf_results = self.content_based_filtering(user_idx, top_k=len(self.products_df))\n",
    "        \n",
    "        # Extract scores\n",
    "        cbf_scores = np.zeros(len(self.products_df))\n",
    "        for result in cbf_results:\n",
    "            cbf_scores[result['product_idx']] = result['similarity_score']\n",
    "        \n",
    "        # Find similar users\n",
    "        similar_users, user_similarities = self.find_similar_users(user_idx, k)\n",
    "        \n",
    "        # Apply aggressive KNN scoring\n",
    "        final_scores = self.aggressive_knn_scoring(cbf_scores, similar_users, k)\n",
    "        \n",
    "        # Get top-N recommendations\n",
    "        top_indices = final_scores.argsort()[-top_n:][::-1]\n",
    "        \n",
    "        recommendations = []\n",
    "        for idx in top_indices:\n",
    "            recommendations.append({\n",
    "                'product_id': self.products_df.iloc[idx]['product_id'],\n",
    "                'name': self.products_df.iloc[idx]['name'],\n",
    "                'cbf_score': cbf_scores[idx],\n",
    "                'final_score': final_scores[idx],\n",
    "                'product_idx': idx\n",
    "            })\n",
    "        \n",
    "        return recommendations, similar_users\n",
    "\n",
    "# Initialize recommendation system\n",
    "rec_system = SkincareRecommendationSystem(products_df, users_df)\n",
    "print(\"‚úÖ Recommendation system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3_checkpoint"
   },
   "source": [
    "### ‚úÖ **Checkpoint 3**: Algorithm Ready!\n",
    "Sistem rekomendasi hybrid telah diimplementasikan dengan:\n",
    "- **Content-Based Filtering** menggunakan TF-IDF\n",
    "- **K-Nearest Neighbors** enhancement\n",
    "- **Aggressive KNN scoring** dengan parameter adaptif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4_header"
   },
   "source": [
    "## üéØ Langkah 4: Test Sistem Rekomendasi\n",
    "\n",
    "Mari kita test sistem dengan contoh pengguna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_system"
   },
   "outputs": [],
   "source": [
    "# Test with sample user\n",
    "test_user_idx = 0\n",
    "test_user = users_df.iloc[test_user_idx]\n",
    "\n",
    "print(f\"üßë‚Äçüíº Testing with User: {test_user['user_id']}\")\n",
    "print(f\"üë§ Profile: {test_user['profile']}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Test with different K values\n",
    "for k in [3, 5, 7]:\n",
    "    print(f\"\\nüîç Testing with K={k}:\")\n",
    "    recommendations, similar_users = rec_system.hybrid_recommend(test_user_idx, k=k, top_n=5)\n",
    "    \n",
    "    print(f\"\\nüìä Top 5 Recommendations (K={k}):\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec['name']} (Score: {rec['final_score']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nüë• Similar Users: {[f'U{idx+1:03d}' for idx in similar_users]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4_checkpoint"
   },
   "source": [
    "### ‚úÖ **Checkpoint 4**: System Working!\n",
    "Sistem rekomendasi berhasil memberikan rekomendasi dengan berbagai nilai K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5_header"
   },
   "source": [
    "## üìã Langkah 5: Generate Ground Truth\n",
    "\n",
    "Kita perlu membuat ground truth untuk evaluasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_ground_truth"
   },
   "outputs": [],
   "source": [
    "def generate_ground_truth(users_df, products_df):\n",
    "    \"\"\"\n",
    "    Generate ground truth based on user-product matching logic\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    \n",
    "    for user_idx, user in users_df.iterrows():\n",
    "        user_skin_type = user['skin_type']\n",
    "        user_concerns = set(user['concerns'].split())\n",
    "        \n",
    "        relevant_products = []\n",
    "        \n",
    "        for prod_idx, product in products_df.iterrows():\n",
    "            # Check skin type compatibility\n",
    "            product_skin_types = set(product['skin_type'].split())\n",
    "            skin_match = user_skin_type in product_skin_types or 'normal' in product_skin_types\n",
    "            \n",
    "            # Check concern overlap\n",
    "            product_concerns = set(product['concerns'].split())\n",
    "            concern_overlap = len(user_concerns.intersection(product_concerns)) > 0\n",
    "            \n",
    "            # Product is relevant if it matches skin type AND addresses concerns\n",
    "            if skin_match and concern_overlap:\n",
    "                # Calculate relevance score based on overlap\n",
    "                overlap_ratio = len(user_concerns.intersection(product_concerns)) / len(user_concerns)\n",
    "                \n",
    "                relevant_products.append({\n",
    "                    'product_idx': prod_idx,\n",
    "                    'product_id': product['product_id'],\n",
    "                    'relevance_score': overlap_ratio\n",
    "                })\n",
    "        \n",
    "        # Sort by relevance score and take top products\n",
    "        relevant_products.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "        \n",
    "        ground_truth[user_idx] = {\n",
    "            'relevant_products': [p['product_idx'] for p in relevant_products],\n",
    "            'relevance_scores': {p['product_idx']: p['relevance_score'] for p in relevant_products}\n",
    "        }\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "# Generate ground truth\n",
    "ground_truth = generate_ground_truth(users_df, products_df)\n",
    "\n",
    "print(\"‚úÖ Ground truth generated!\")\n",
    "print(f\"\\nüìä Ground Truth Statistics:\")\n",
    "for user_idx in range(min(5, len(users_df))):\n",
    "    n_relevant = len(ground_truth[user_idx]['relevant_products'])\n",
    "    print(f\"User {user_idx+1}: {n_relevant} relevant products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5_checkpoint"
   },
   "source": [
    "### ‚úÖ **Checkpoint 5**: Ground Truth Ready!\n",
    "Ground truth telah dibuat berdasarkan kecocokan skin type dan concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6_header"
   },
   "source": [
    "## üìä Langkah 6: Implementasi Metrik Evaluasi\n",
    "\n",
    "Sekarang kita akan mengimplementasikan berbagai metrik evaluasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation_metrics"
   },
   "outputs": [],
   "source": [
    "class RecommendationEvaluator:\n",
    "    def __init__(self, rec_system, ground_truth):\n",
    "        self.rec_system = rec_system\n",
    "        self.ground_truth = ground_truth\n",
    "    \n",
    "    def precision_at_k(self, recommended_items, relevant_items, k):\n",
    "        \"\"\"\n",
    "        Calculate Precision@K\n",
    "        \"\"\"\n",
    "        if k == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        recommended_k = recommended_items[:k]\n",
    "        relevant_set = set(relevant_items)\n",
    "        \n",
    "        hits = len([item for item in recommended_k if item in relevant_set])\n",
    "        return hits / k\n",
    "    \n",
    "    def recall_at_k(self, recommended_items, relevant_items, k):\n",
    "        \"\"\"\n",
    "        Calculate Recall@K\n",
    "        \"\"\"\n",
    "        if len(relevant_items) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        recommended_k = recommended_items[:k]\n",
    "        relevant_set = set(relevant_items)\n",
    "        \n",
    "        hits = len([item for item in recommended_k if item in relevant_set])\n",
    "        return hits / len(relevant_items)\n",
    "    \n",
    "    def f1_score_at_k(self, recommended_items, relevant_items, k):\n",
    "        \"\"\"\n",
    "        Calculate F1-Score@K\n",
    "        \"\"\"\n",
    "        precision = self.precision_at_k(recommended_items, relevant_items, k)\n",
    "        recall = self.recall_at_k(recommended_items, relevant_items, k)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    def ndcg_at_k(self, recommended_items, relevant_items, relevance_scores, k):\n",
    "        \"\"\"\n",
    "        Calculate NDCG@K\n",
    "        \"\"\"\n",
    "        def dcg_at_k(scores, k):\n",
    "            scores = np.array(scores[:k])\n",
    "            if scores.size == 0:\n",
    "                return 0.0\n",
    "            return scores[0] + np.sum(scores[1:] / np.log2(np.arange(2, scores.size + 1)))\n",
    "        \n",
    "        # Get relevance scores for recommended items\n",
    "        recommended_k = recommended_items[:k]\n",
    "        actual_scores = [relevance_scores.get(item, 0.0) for item in recommended_k]\n",
    "        \n",
    "        # Calculate DCG\n",
    "        dcg = dcg_at_k(actual_scores, k)\n",
    "        \n",
    "        # Calculate IDCG (ideal DCG)\n",
    "        ideal_scores = sorted([relevance_scores.get(item, 0.0) for item in relevant_items], reverse=True)\n",
    "        idcg = dcg_at_k(ideal_scores, k)\n",
    "        \n",
    "        if idcg == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return dcg / idcg\n",
    "    \n",
    "    def average_precision(self, recommended_items, relevant_items):\n",
    "        \"\"\"\n",
    "        Calculate Average Precision\n",
    "        \"\"\"\n",
    "        if len(relevant_items) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        relevant_set = set(relevant_items)\n",
    "        hits = 0\n",
    "        sum_precisions = 0.0\n",
    "        \n",
    "        for i, item in enumerate(recommended_items):\n",
    "            if item in relevant_set:\n",
    "                hits += 1\n",
    "                precision_at_i = hits / (i + 1)\n",
    "                sum_precisions += precision_at_i\n",
    "        \n",
    "        return sum_precisions / len(relevant_items)\n",
    "    \n",
    "    def evaluate_user(self, user_idx, k_neighbors, k_values=[3, 5, 7, 10]):\n",
    "        \"\"\"\n",
    "        Evaluate recommendations for a single user\n",
    "        \"\"\"\n",
    "        # Get recommendations\n",
    "        recommendations, _ = self.rec_system.hybrid_recommend(user_idx, k=k_neighbors, top_n=max(k_values))\n",
    "        recommended_items = [rec['product_idx'] for rec in recommendations]\n",
    "        \n",
    "        # Get ground truth\n",
    "        relevant_items = self.ground_truth[user_idx]['relevant_products']\n",
    "        relevance_scores = self.ground_truth[user_idx]['relevance_scores']\n",
    "        \n",
    "        # Calculate metrics for different K values\n",
    "        metrics = {}\n",
    "        for k in k_values:\n",
    "            metrics[k] = {\n",
    "                'precision': self.precision_at_k(recommended_items, relevant_items, k),\n",
    "                'recall': self.recall_at_k(recommended_items, relevant_items, k),\n",
    "                'f1_score': self.f1_score_at_k(recommended_items, relevant_items, k),\n",
    "                'ndcg': self.ndcg_at_k(recommended_items, relevant_items, relevance_scores, k)\n",
    "            }\n",
    "        \n",
    "        # Calculate MAP\n",
    "        map_score = self.average_precision(recommended_items, relevant_items)\n",
    "        \n",
    "        return metrics, map_score\n",
    "    \n",
    "    def evaluate_all_users(self, k_neighbors, k_values=[3, 5, 7, 10]):\n",
    "        \"\"\"\n",
    "        Evaluate recommendations for all users\n",
    "        \"\"\"\n",
    "        all_metrics = {k: {'precision': [], 'recall': [], 'f1_score': [], 'ndcg': []} for k in k_values}\n",
    "        all_map_scores = []\n",
    "        \n",
    "        for user_idx in range(len(self.rec_system.users_df)):\n",
    "            user_metrics, map_score = self.evaluate_user(user_idx, k_neighbors, k_values)\n",
    "            \n",
    "            for k in k_values:\n",
    "                for metric in ['precision', 'recall', 'f1_score', 'ndcg']:\n",
    "                    all_metrics[k][metric].append(user_metrics[k][metric])\n",
    "            \n",
    "            all_map_scores.append(map_score)\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_metrics = {}\n",
    "        for k in k_values:\n",
    "            avg_metrics[k] = {}\n",
    "            for metric in ['precision', 'recall', 'f1_score', 'ndcg']:\n",
    "                avg_metrics[k][metric] = np.mean(all_metrics[k][metric])\n",
    "        \n",
    "        avg_map = np.mean(all_map_scores)\n",
    "        \n",
    "        return avg_metrics, avg_map\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RecommendationEvaluator(rec_system, ground_truth)\n",
    "print(\"‚úÖ Evaluator initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6_checkpoint"
   },
   "source": [
    "### ‚úÖ **Checkpoint 6**: Metrics Ready!\n",
    "Semua metrik evaluasi telah diimplementasikan:\n",
    "- **Precision@K, Recall@K, F1-Score@K**\n",
    "- **NDCG@K** untuk kualitas ranking\n",
    "- **MAP** untuk performa keseluruhan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step7_header"
   },
   "source": [
    "## üîç Langkah 7: Eksekusi Evaluasi Lengkap\n",
    "\n",
    "Sekarang kita akan menjalankan evaluasi lengkap dengan berbagai nilai K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "full_evaluation"
   },
   "outputs": [],
   "source": [
    "# Run full evaluation\n",
    "k_neighbors_list = [3, 5, 7, 10]\n",
    "k_values = [3, 5, 7, 10]\n",
    "\n",
    "print(\"üîç Running Full Evaluation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for k_neighbors in k_neighbors_list:\n",
    "    print(f\"\\nüìä Evaluating with K_neighbors = {k_neighbors}\")\n",
    "    \n",
    "    avg_metrics, avg_map = evaluator.evaluate_all_users(k_neighbors, k_values)\n",
    "    \n",
    "    evaluation_results[k_neighbors] = {\n",
    "        'metrics': avg_metrics,\n",
    "        'map': avg_map\n",
    "    }\n",
    "    \n",
    "    print(f\"MAP: {avg_map:.3f}\")\n",
    "    \n",
    "    # Display results for each K\n",
    "    for k in k_values:\n",
    "        metrics = avg_metrics[k]\n",
    "        print(f\"  K={k}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}, NDCG={metrics['ndcg']:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "results_summary"
   },
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "print(\"üìä COMPREHENSIVE EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_data = []\n",
    "for k_neighbors in k_neighbors_list:\n",
    "    for k in k_values:\n",
    "        metrics = evaluation_results[k_neighbors]['metrics'][k]\n",
    "        results_data.append({\n",
    "            'K_neighbors': k_neighbors,\n",
    "            'K_eval': k,\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1_Score': metrics['f1_score'],\n",
    "            'NDCG': metrics['ndcg'],\n",
    "            'MAP': evaluation_results[k_neighbors]['map']\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\nüìã Detailed Results Table:\")\n",
    "print(results_df.round(3))\n",
    "\n",
    "# Find best performing configurations\n",
    "print(\"\\nüèÜ BEST PERFORMING CONFIGURATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for metric in ['F1_Score', 'Precision', 'Recall', 'NDCG']:\n",
    "    best_idx = results_df[metric].idxmax()\n",
    "    best_config = results_df.iloc[best_idx]\n",
    "    print(f\"Best {metric}: K_neighbors={best_config['K_neighbors']}, K_eval={best_config['K_eval']}, Score={best_config[metric]:.3f}\")\n",
    "\n",
    "best_map_k = max(evaluation_results.keys(), key=lambda x: evaluation_results[x]['map'])\n",
    "print(f\"Best MAP: K_neighbors={best_map_k}, Score={evaluation_results[best_map_k]['map']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step7_checkpoint"
   },
   "source": [
    "### ‚úÖ **Checkpoint 7**: Evaluation Complete!\n",
    "Evaluasi lengkap telah selesai dengan berbagai kombinasi K values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step8_header"
   },
   "source": [
    "## üìà Langkah 8: Visualisasi dan Analisis Hasil\n",
    "\n",
    "Mari kita visualisasikan dan analisis hasil evaluasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization"
   },
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('üß¥ Skincare Recommendation Algorithm Evaluation Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1_Score', 'NDCG']\n",
    "k_eval_values = [3, 5, 7, 10]\n",
    "\n",
    "# Plot 1: Precision@K for different K_neighbors\n",
    "ax1 = axes[0, 0]\n",
    "for k_neighbors in k_neighbors_list:\n",
    "    precision_values = [evaluation_results[k_neighbors]['metrics'][k]['precision'] for k in k_eval_values]\n",
    "    ax1.plot(k_eval_values, precision_values, marker='o', linewidth=2, label=f'K_neighbors={k_neighbors}')\n",
    "ax1.set_title('Precision@K', fontweight='bold')\n",
    "ax1.set_xlabel('K (Evaluation)')\n",
    "ax1.set_ylabel('Precision')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Recall@K for different K_neighbors\n",
    "ax2 = axes[0, 1]\n",
    "for k_neighbors in k_neighbors_list:\n",
    "    recall_values = [evaluation_results[k_neighbors]['metrics'][k]['recall'] for k in k_eval_values]\n",
    "    ax2.plot(k_eval_values, recall_values, marker='s', linewidth=2, label=f'K_neighbors={k_neighbors}')\n",
    "ax2.set_title('Recall@K', fontweight='bold')\n",
    "ax2.set_xlabel('K (Evaluation)')\n",
    "ax2.set_ylabel('Recall')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: F1-Score@K for different K_neighbors\n",
    "ax3 = axes[0, 2]\n",
    "for k_neighbors in k_neighbors_list:\n",
    "    f1_values = [evaluation_results[k_neighbors]['metrics'][k]['f1_score'] for k in k_eval_values]\n",
    "    ax3.plot(k_eval_values, f1_values, marker='^', linewidth=2, label=f'K_neighbors={k_neighbors}')\n",
    "ax3.set_title('F1-Score@K', fontweight='bold')\n",
    "ax3.set_xlabel('K (Evaluation)')\n",
    "ax3.set_ylabel('F1-Score')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: NDCG@K for different K_neighbors\n",
    "ax4 = axes[1, 0]\n",
    "for k_neighbors in k_neighbors_list:\n",
    "    ndcg_values = [evaluation_results[k_neighbors]['metrics'][k]['ndcg'] for k in k_eval_values]\n",
    "    ax4.plot(k_eval_values, ndcg_values, marker='d', linewidth=2, label=f'K_neighbors={k_neighbors}')\n",
    "ax4.set_title('NDCG@K', fontweight='bold')\n",
    "ax4.set_xlabel('K (Evaluation)')\n",
    "ax4.set_ylabel('NDCG')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: MAP for different K_neighbors\n",
    "ax5 = axes[1, 1]\n",
    "map_values = [evaluation_results[k]['map'] for k in k_neighbors_list]\n",
    "bars = ax5.bar([str(k) for k in k_neighbors_list], map_values, color='skyblue', alpha=0.7)\n",
    "ax5.set_title('Mean Average Precision (MAP)', fontweight='bold')\n",
    "ax5.set_xlabel('K_neighbors')\n",
    "ax5.set_ylabel('MAP')\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, map_values):\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 6: Heatmap of F1-Scores\n",
    "ax6 = axes[1, 2]\n",
    "f1_matrix = np.zeros((len(k_neighbors_list), len(k_eval_values)))\n",
    "for i, k_neighbors in enumerate(k_neighbors_list):\n",
    "    for j, k_eval in enumerate(k_eval_values):\n",
    "        f1_matrix[i, j] = evaluation_results[k_neighbors]['metrics'][k_eval]['f1_score']\n",
    "\n",
    "im = ax6.imshow(f1_matrix, cmap='YlOrRd', aspect='auto')\n",
    "ax6.set_title('F1-Score Heatmap', fontweight='bold')\n",
    "ax6.set_xlabel('K (Evaluation)')\n",
    "ax6.set_ylabel('K_neighbors')\n",
    "ax6.set_xticks(range(len(k_eval_values)))\n",
    "ax6.set_xticklabels(k_eval_values)\n",
    "ax6.set_yticks(range(len(k_neighbors_list)))\n",
    "ax6.set_yticklabels(k_neighbors_list)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(k_neighbors_list)):\n",
    "    for j in range(len(k_eval_values)):\n",
    "        ax6.text(j, i, f'{f1_matrix[i, j]:.3f}', ha='center', va='center', \n",
    "                color='white' if f1_matrix[i, j] > 0.5 else 'black', fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax6, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "detailed_analysis"
   },
   "outputs": [],
   "source": [
    "# Detailed Performance Analysis\n",
    "print(\"üîç DETAILED PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# K-Sensitivity Analysis\n",
    "print(\"\\nüìà K-SENSITIVITY ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for k_neighbors in k_neighbors_list:\n",
    "    f1_scores = [evaluation_results[k_neighbors]['metrics'][k]['f1_score'] for k in k_eval_values]\n",
    "    f1_variance = np.var(f1_scores)\n",
    "    f1_mean = np.mean(f1_scores)\n",
    "    \n",
    "    print(f\"\\nK_neighbors = {k_neighbors}:\")\n",
    "    print(f\"  üìä F1-Score Mean: {f1_mean:.3f}\")\n",
    "    print(f\"  üìä F1-Score Variance: {f1_variance:.6f}\")\n",
    "    \n",
    "    if f1_variance < 0.001:\n",
    "        stability = \"Very Stable\"\n",
    "    elif f1_variance < 0.005:\n",
    "        stability = \"Stable\"\n",
    "    else:\n",
    "        stability = \"Sensitive\"\n",
    "    \n",
    "    print(f\"  üéØ Stability: {stability}\")\n",
    "\n",
    "# Performance Ranking\n",
    "print(\"\\nüèÜ PERFORMANCE RANKING:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Rank by average F1-Score across all K values\n",
    "k_performance = []\n",
    "for k_neighbors in k_neighbors_list:\n",
    "    avg_f1 = np.mean([evaluation_results[k_neighbors]['metrics'][k]['f1_score'] for k in k_eval_values])\n",
    "    k_performance.append((k_neighbors, avg_f1))\n",
    "\n",
    "k_performance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for rank, (k_neighbors, avg_f1) in enumerate(k_performance, 1):\n",
    "    print(f\"{rank}. K_neighbors = {k_neighbors}: Avg F1-Score = {avg_f1:.3f}\")\n",
    "\n",
    "# Best configuration summary\n",
    "best_k = k_performance[0][0]\n",
    "best_metrics = evaluation_results[best_k]['metrics']\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDED CONFIGURATION:\")\n",
    "print(f\"   üîß Optimal K_neighbors: {best_k}\")\n",
    "print(f\"   üìä Performance at K=5:\")\n",
    "print(f\"      ‚Ä¢ Precision@5: {best_metrics[5]['precision']:.3f}\")\n",
    "print(f\"      ‚Ä¢ Recall@5: {best_metrics[5]['recall']:.3f}\")\n",
    "print(f\"      ‚Ä¢ F1-Score@5: {best_metrics[5]['f1_score']:.3f}\")\n",
    "print(f\"      ‚Ä¢ NDCG@5: {best_metrics[5]['ndcg']:.3f}\")\n",
    "print(f\"   üìä MAP: {evaluation_results[best_k]['map']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step8_checkpoint"
   },
   "source": [
    "### ‚úÖ **Checkpoint 8**: Analysis Complete!\n",
    "Visualisasi dan analisis mendalam telah selesai:\n",
    "- **Grafik performa** untuk semua metrik\n",
    "- **K-sensitivity analysis** untuk stabilitas\n",
    "- **Performance ranking** dan rekomendasi konfigurasi optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step9_header"
   },
   "source": [
    "## üìã Langkah 9: Kesimpulan dan Rekomendasi\n",
    "\n",
    "Mari kita buat kesimpulan akhir dari evaluasi yang telah dilakukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_conclusions"
   },
   "outputs": [],
   "source": [
    "# Final Conclusions and Recommendations\n",
    "print(\"üìã KESIMPULAN AKHIR EVALUASI ALGORITMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find overall best performing K\n",
    "best_k = max(evaluation_results.keys(), \n",
    "            key=lambda x: np.mean([evaluation_results[x]['metrics'][k]['f1_score'] for k in k_eval_values]))\n",
    "best_metrics = evaluation_results[best_k]['metrics']\n",
    "\n",
    "print(f\"\\nüèÜ HASIL TERBAIK:\")\n",
    "print(f\"   üéØ Nilai K Optimal: K={best_k}\")\n",
    "print(f\"   üìä F1-Score@5: {best_metrics[5]['f1_score']:.3f}\")\n",
    "print(f\"   üìä Precision@5: {best_metrics[5]['precision']:.3f}\")\n",
    "print(f\"   üìä Recall@5: {best_metrics[5]['recall']:.3f}\")\n",
    "print(f\"   üìä NDCG@5: {best_metrics[5]['ndcg']:.3f}\")\n",
    "print(f\"   üìä MAP: {evaluation_results[best_k]['map']:.3f}\")\n",
    "\n",
    "# Performance interpretation\n",
    "print(f\"\\nüí° INTERPRETASI HASIL:\")\n",
    "f1_score = best_metrics[5]['f1_score']\n",
    "if f1_score >= 0.7:\n",
    "    performance_level = \"Sangat Baik\"\n",
    "    performance_desc = \"Algoritma menunjukkan performa yang sangat baik\"\n",
    "elif f1_score >= 0.5:\n",
    "    performance_level = \"Baik\"\n",
    "    performance_desc = \"Algoritma menunjukkan performa yang baik dan dapat diandalkan\"\n",
    "elif f1_score >= 0.3:\n",
    "    performance_level = \"Cukup\"\n",
    "    performance_desc = \"Algoritma menunjukkan performa yang cukup, masih dapat ditingkatkan\"\n",
    "else:\n",
    "    performance_level = \"Perlu Perbaikan\"\n",
    "    performance_desc = \"Algoritma memerlukan perbaikan signifikan\"\n",
    "\n",
    "print(f\"   üìà Level Performa: {performance_level}\")\n",
    "print(f\"   üìù Deskripsi: {performance_desc}\")\n",
    "\n",
    "# Algorithm characteristics\n",
    "precision = best_metrics[5]['precision']\n",
    "recall = best_metrics[5]['recall']\n",
    "precision_recall_ratio = precision / recall if recall > 0 else 0\n",
    "\n",
    "print(f\"\\nüîç KARAKTERISTIK ALGORITMA:\")\n",
    "if precision_recall_ratio > 1.2:\n",
    "    print(f\"   üéØ Algoritma cenderung lebih akurat (precision-focused)\")\n",
    "    print(f\"   üíº Cocok untuk aplikasi yang mengutamakan akurasi rekomendasi\")\n",
    "elif precision_recall_ratio < 0.8:\n",
    "    print(f\"   üìä Algoritma cenderung lebih komprehensif (recall-focused)\")\n",
    "    print(f\"   üíº Cocok untuk aplikasi yang mengutamakan kelengkapan rekomendasi\")\n",
    "else:\n",
    "    print(f\"   ‚öñÔ∏è Algoritma memiliki keseimbangan yang baik antara akurasi dan kelengkapan\")\n",
    "    print(f\"   üíº Cocok untuk aplikasi umum yang membutuhkan performa seimbang\")\n",
    "\n",
    "print(f\"\\nüöÄ REKOMENDASI IMPLEMENTASI:\")\n",
    "print(f\"   ‚Ä¢ Gunakan K_neighbors = {best_k} untuk hasil optimal\")\n",
    "print(f\"   ‚Ä¢ Monitor performa secara berkala\")\n",
    "print(f\"   ‚Ä¢ Validasi dengan data real pengguna\")\n",
    "print(f\"   ‚Ä¢ Pertimbangkan A/B testing untuk validasi\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Evaluasi algoritma rekomendasi skincare selesai!\")\n",
    "print(\"üìä Semua metrik telah dievaluasi dan dianalisis.\")\n",
    "print(\"üéØ Rekomendasi konfigurasi optimal telah diberikan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final_checkpoint"
   },
   "source": [
    "### ‚úÖ **Checkpoint 9**: Evaluation Complete!\n",
    "\n",
    "üéâ **Selamat!** Anda telah berhasil menyelesaikan evaluasi algoritma rekomendasi skincare secara lengkap!\n",
    "\n",
    "## üìä **Ringkasan Pencapaian:**\n",
    "- ‚úÖ **Environment Setup** - Library dan dependencies siap\n",
    "- ‚úÖ **Data Generation** - Dataset sintetis 50 produk & 20 pengguna\n",
    "- ‚úÖ **Algorithm Implementation** - Hybrid CBF+KNN system\n",
    "- ‚úÖ **Ground Truth Creation** - Data referensi untuk evaluasi\n",
    "- ‚úÖ **Metrics Implementation** - Precision, Recall, F1, NDCG, MAP\n",
    "- ‚úÖ **Full Evaluation** - Testing dengan berbagai nilai K\n",
    "- ‚úÖ **Visualization & Analysis** - Grafik dan insight mendalam\n",
    "- ‚úÖ **Conclusions & Recommendations** - Panduan implementasi\n",
    "\n",
    "## üéØ **Hasil Utama:**\n",
    "- **Nilai K optimal** telah diidentifikasi\n",
    "- **Karakteristik algoritma** telah dianalisis\n",
    "- **Panduan implementasi** telah disediakan\n",
    "- **Metrik performa** telah dievaluasi secara komprehensif\n",
    "\n",
    "## üöÄ **Langkah Selanjutnya:**\n",
    "1. **Implementasi** sistem dengan nilai K yang direkomendasikan\n",
    "2. **Testing** dengan data real pengguna\n",
    "3. **Monitoring** performa di production\n",
    "4. **Iterasi** berdasarkan feedback pengguna\n",
    "\n",
    "---\n",
    "**Terima kasih telah mengikuti evaluasi ini! üôè**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "appendix_header"
   },
   "source": [
    "## üìö Appendix: Informasi Tambahan\n",
    "\n",
    "### üîç **Penjelasan Metrik Evaluasi:**\n",
    "\n",
    "**Precision@K**: Mengukur proporsi item relevan dalam K rekomendasi teratas\n",
    "- Formula: `Relevant items in top-K / K`\n",
    "- Interpretasi: Seberapa akurat rekomendasi yang diberikan\n",
    "\n",
    "**Recall@K**: Mengukur proporsi item relevan yang berhasil ditangkap dalam K rekomendasi\n",
    "- Formula: `Relevant items in top-K / Total relevant items`\n",
    "- Interpretasi: Seberapa lengkap rekomendasi menangkap item relevan\n",
    "\n",
    "**F1-Score@K**: Harmonic mean dari Precision dan Recall\n",
    "- Formula: `2 * (Precision * Recall) / (Precision + Recall)`\n",
    "- Interpretasi: Keseimbangan antara akurasi dan kelengkapan\n",
    "\n",
    "**NDCG@K**: Normalized Discounted Cumulative Gain\n",
    "- Mengukur kualitas ranking dengan mempertimbangkan posisi\n",
    "- Interpretasi: Seberapa baik urutan rekomendasi\n",
    "\n",
    "**MAP**: Mean Average Precision\n",
    "- Rata-rata dari Average Precision semua pengguna\n",
    "- Interpretasi: Performa keseluruhan sistem rekomendasi\n",
    "\n",
    "### ü§ñ **Cara Kerja Algoritma Hybrid CBF+KNN:**\n",
    "\n",
    "1. **Content-Based Filtering**: Menghitung similarity antara profil pengguna dan produk menggunakan TF-IDF\n",
    "2. **K-Nearest Neighbors**: Mencari K pengguna terdekat berdasarkan kesamaan preferensi\n",
    "3. **Hybrid Scoring**: Menggabungkan skor CBF dengan boost dari KNN\n",
    "4. **Ranking**: Mengurutkan produk berdasarkan skor akhir\n",
    "\n",
    "### üìä **Interpretasi Nilai K:**\n",
    "\n",
    "- **K kecil (3-5)**: Lebih selektif, fokus pada pengguna yang sangat mirip\n",
    "- **K sedang (5-7)**: Keseimbangan antara selektivitas dan diversitas\n",
    "- **K besar (7-10)**: Lebih inklusif, mempertimbangkan lebih banyak pengguna\n",
    "\n",
    "### ‚ö†Ô∏è **Limitasi Evaluasi:**\n",
    "\n",
    "1. **Dataset Sintetis**: Hasil mungkin berbeda dengan data real\n",
    "2. **Ground Truth Sederhana**: Menggunakan aturan matching yang basic\n",
    "3. **Skala Kecil**: 50 produk dan 20 pengguna untuk demo\n",
    "4. **Tanpa Temporal Factor**: Tidak mempertimbangkan perubahan preferensi waktu\n",
    "\n",
    "### üîß **Tips Implementasi Production:**\n",
    "\n",
    "1. **Scalability**: Gunakan approximate nearest neighbor untuk dataset besar\n",
    "2. **Real-time**: Implementasi caching untuk response time yang cepat\n",
    "3. **Cold Start**: Siapkan strategi untuk pengguna/produk baru\n",
    "4. **Feedback Loop**: Implementasi implicit/explicit feedback collection\n",
    "5. **A/B Testing**: Validasi performa dengan eksperimen terkontrol"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}